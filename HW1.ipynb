{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbc87498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets evaluate scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d4d1b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Use this only after you check everything is being loaded properly\n",
    "\n",
    "import torch\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,\\\n",
    "    TrainingArguments, Trainer, pipeline, DataCollatorWithPadding, set_seed\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "# classifier(['this is a bad idea', 'I hate this so much'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25812ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99284ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dynasent (C:/Users/Jeonghoon Kim/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r1.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057a34590c2a4efdbe519be53b19524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dynasent (C:/Users/Jeonghoon Kim/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r2.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5436243035694d15b88dd202428a8db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r1_dataset = load_dataset(\"dynabench/dynasent\", \"dynabench.dynasent.r1.all\")\n",
    "\n",
    "r2_dataset = load_dataset(\"dynabench/dynasent\", \"dynabench.dynasent.r2.all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5287bafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8aa47551744606aabef868464d3ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/81 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb31535a40b54a41be81e118d19d1815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e07792c024105868f124732e01c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# string label to int\n",
    "r1_dataset = r1_dataset.class_encode_column('gold_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2dce96da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 80488\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
       "        num_rows: 3600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c15815c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'r1-0098060',\n",
       " 'hit_ids': ['y14605', 'y14606'],\n",
       " 'sentence': 'Had to remind him to toast the sandwich.',\n",
       " 'indices_into_review_text': [700, 740],\n",
       " 'model_0_label': 'positive',\n",
       " 'model_0_probs': {'negative': 0.011094148270785809,\n",
       "  'positive': 0.7560697793960571,\n",
       "  'neutral': 0.2328360378742218},\n",
       " 'text_id': 'r1-0098060',\n",
       " 'review_id': 'ebgPQgQJx5Al2CC-aNMU5A',\n",
       " 'review_rating': 1,\n",
       " 'label_distribution': {'positive': [],\n",
       "  'negative': ['w114', 'w380', 'w40', 'w516'],\n",
       "  'neutral': ['w1269'],\n",
       "  'mixed': []},\n",
       " 'gold_label': 0,\n",
       " 'metadata': {'split': 'test',\n",
       "  'round': 1,\n",
       "  'subset': 'all',\n",
       "  'model_in_the_loop': 'RoBERTa'}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29e6831e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'r2-0019256',\n",
       " 'hit_ids': ['y21512', 'y21524'],\n",
       " 'sentence': 'The art exhibit has a lot to offer.',\n",
       " 'sentence_author': 'w262',\n",
       " 'has_prompt': True,\n",
       " 'prompt_data': {'indices_into_review_text': [242, 356],\n",
       "  'review_rating': 5,\n",
       "  'prompt_sentence': \"They're currently under construction for a new exhibit, but there is still enough art to enjoy for around 2 hours.\",\n",
       "  'review_id': '0cJld_mdcScG6zZtoPEFTA'},\n",
       " 'model_1_label': 'positive',\n",
       " 'model_1_probs': {'negative': 0.010349077172577381,\n",
       "  'positive': 0.8954706788063049,\n",
       "  'neutral': 0.09418027848005295},\n",
       " 'text_id': 'r2-0019256',\n",
       " 'label_distribution': {'positive': ['w148', 'w358', 'w4', 'w423', 'w139'],\n",
       "  'negative': [],\n",
       "  'neutral': [],\n",
       "  'mixed': []},\n",
       " 'gold_label': 'positive',\n",
       " 'metadata': {'split': 'test',\n",
       "  'round': 2,\n",
       "  'subset': 'all',\n",
       "  'model_in_the_loop': 'RoBERTa'}}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "526d62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1_dataset = r1_dataset.rename_column('gold_label', 'label')\n",
    "r2_dataset = r2_dataset.rename_column('gold_label', 'label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c9978fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['r1-0000001',\n",
       "  'r1-0000002',\n",
       "  'r1-0000003',\n",
       "  'r1-0000004',\n",
       "  'r1-0000006',\n",
       "  'r1-0000007',\n",
       "  'r1-0000008',\n",
       "  'r1-0000010',\n",
       "  'r1-0000011',\n",
       "  'r1-0000012'],\n",
       " 'hit_ids': [['y5238'],\n",
       "  ['y11155'],\n",
       "  ['y14984', 'y17992'],\n",
       "  ['y1167', 'y297'],\n",
       "  ['y11412', 'y19170', 'y4073'],\n",
       "  ['y2002'],\n",
       "  ['y14885', 'y16948'],\n",
       "  ['y18243', 'y4069', 'y4712'],\n",
       "  ['y19430', 'y2698'],\n",
       "  ['y3416']],\n",
       " 'sentence': ['Roto-Rooter is always good when you need someone right away.',\n",
       "  \"It's so worth the price of cox service over headaches of not being able to publish anything in a short amount of time on the Internet or frankly just stream in different rooms without buffering constantly.\",\n",
       "  'I placed my order of \"sticky ribs\" as an appetizer and the \"angry chicken\" as my entree.',\n",
       "  'There is mandatory valet parking, so make sure you get everything you need from the car!',\n",
       "  \"My wife and I couldn't finish it.\",\n",
       "  'I went with a revised quote and they set an appointment for the next week.',\n",
       "  'I found out that the restaurant has only been open for about a week.',\n",
       "  'I suffer with high blood pressure, headaches, type 2 diabetes and TMJ.',\n",
       "  \"Seriously, come here now before the hype sets in and you won't be able to get a table!\",\n",
       "  'It was difficult to find the place.'],\n",
       " 'indices_into_review_text': [[0, 60],\n",
       "  [155, 360],\n",
       "  [530, 618],\n",
       "  [798, 886],\n",
       "  [294, 327],\n",
       "  [732, 806],\n",
       "  [138, 206],\n",
       "  [449, 519],\n",
       "  [1060, 1146],\n",
       "  [86, 121]],\n",
       " 'model_0_label': ['positive',\n",
       "  'positive',\n",
       "  'negative',\n",
       "  'positive',\n",
       "  'negative',\n",
       "  'negative',\n",
       "  'negative',\n",
       "  'neutral',\n",
       "  'positive',\n",
       "  'neutral'],\n",
       " 'model_0_probs': [{'negative': 0.01173639390617609,\n",
       "   'positive': 0.7473671436309814,\n",
       "   'neutral': 0.24089649319648743},\n",
       "  {'negative': 0.004590339958667755,\n",
       "   'positive': 0.950273871421814,\n",
       "   'neutral': 0.04513582959771156},\n",
       "  {'negative': 0.9451407194137573,\n",
       "   'positive': 0.004023305606096983,\n",
       "   'neutral': 0.05083595588803291},\n",
       "  {'negative': 0.0416930615901947,\n",
       "   'positive': 0.5428488254547119,\n",
       "   'neutral': 0.4154581129550934},\n",
       "  {'negative': 0.977493166923523,\n",
       "   'positive': 0.0006850706995464861,\n",
       "   'neutral': 0.02182171680033207},\n",
       "  {'negative': 0.6572147011756897,\n",
       "   'positive': 0.09853064268827438,\n",
       "   'neutral': 0.2442547082901001},\n",
       "  {'negative': 0.45139288902282715,\n",
       "   'positive': 0.12103912234306335,\n",
       "   'neutral': 0.4275679886341095},\n",
       "  {'negative': 0.3392733931541443,\n",
       "   'positive': 0.30037474632263184,\n",
       "   'neutral': 0.36035192012786865},\n",
       "  {'negative': 0.026026399806141853,\n",
       "   'positive': 0.8168579936027527,\n",
       "   'neutral': 0.15711559355258942},\n",
       "  {'negative': 0.2690688669681549,\n",
       "   'positive': 0.2074701339006424,\n",
       "   'neutral': 0.5234610438346863}],\n",
       " 'text_id': ['r1-0000001',\n",
       "  'r1-0000002',\n",
       "  'r1-0000003',\n",
       "  'r1-0000004',\n",
       "  'r1-0000006',\n",
       "  'r1-0000007',\n",
       "  'r1-0000008',\n",
       "  'r1-0000010',\n",
       "  'r1-0000011',\n",
       "  'r1-0000012'],\n",
       " 'review_id': ['IDHkeGo-nxhqX4Exkdr08A',\n",
       "  'HERx4qHWIVDwH3AME9OYgQ',\n",
       "  'gCshZ3zPCaAwu8sAejmTcA',\n",
       "  'JeLaSy9T3-pdVj_fQ6JX9A',\n",
       "  'o-5hLEypDxR4jXjbmzkNBA',\n",
       "  'jttEKPQZ6UImUQVubG1GDg',\n",
       "  'W8GvEfQ2qShtF7iRmehWVw',\n",
       "  'uKrOYLnAVUsB1GE2huJt4A',\n",
       "  '80r8He9zm4G1aR5N_7EP3Q',\n",
       "  'B6mQSD5qLmd0kaaTzRh8jw'],\n",
       " 'review_rating': [1, 1, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       " 'label_distribution': [{'positive': ['w130', 'w186', 'w207', 'w264', 'w54'],\n",
       "   'negative': [],\n",
       "   'neutral': [],\n",
       "   'mixed': []},\n",
       "  {'positive': ['w129', 'w375', 'w39'],\n",
       "   'negative': ['w70'],\n",
       "   'neutral': [],\n",
       "   'mixed': ['w477']},\n",
       "  {'positive': ['w133'],\n",
       "   'negative': [],\n",
       "   'neutral': ['w135', 'w184', 'w38', 'w439'],\n",
       "   'mixed': []},\n",
       "  {'positive': [],\n",
       "   'negative': ['w135'],\n",
       "   'neutral': ['w148', 'w207', 'w294', 'w772'],\n",
       "   'mixed': []},\n",
       "  {'positive': [],\n",
       "   'negative': ['w254'],\n",
       "   'neutral': ['w176', 'w328', 'w39', 'w598'],\n",
       "   'mixed': []},\n",
       "  {'positive': ['w280'],\n",
       "   'negative': [],\n",
       "   'neutral': ['w265', 'w46', 'w490', 'w936'],\n",
       "   'mixed': []},\n",
       "  {'positive': [],\n",
       "   'negative': [],\n",
       "   'neutral': ['w137', 'w310', 'w415', 'w460', 'w627'],\n",
       "   'mixed': []},\n",
       "  {'positive': [],\n",
       "   'negative': ['w155', 'w193', 'w204', 'w620'],\n",
       "   'neutral': ['w133'],\n",
       "   'mixed': []},\n",
       "  {'positive': ['w132', 'w173', 'w227'],\n",
       "   'negative': [],\n",
       "   'neutral': ['w447'],\n",
       "   'mixed': ['w23']},\n",
       "  {'positive': ['w385', 'w591'],\n",
       "   'negative': ['w290', 'w539', 'w552'],\n",
       "   'neutral': [],\n",
       "   'mixed': []}],\n",
       " 'label': [2, 2, 1, 1, 1, 1, 1, 0, 2, 0],\n",
       " 'metadata': [{'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'},\n",
       "  {'split': 'train',\n",
       "   'round': 1,\n",
       "   'subset': 'all',\n",
       "   'model_in_the_loop': 'RoBERTa'}]}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c3c82",
   "metadata": {},
   "source": [
    "There are two fields in this dataset: \n",
    "\n",
    "- 'hit_ids': List of Amazon Mechanical Turk Human Interface Tasks (HITs) in which this example appeared during validation. The values are anonymized but used consistently throughout the dataset.\n",
    "- 'sentence': The example text.\n",
    "- 'indices_into_review_text': indices of 'sentence' into the original review in the Yelp Academic Dataset.\n",
    "- 'model_0_label': prediction of Model 0 as described in the paper. The possible values are 'positive', 'negative', and 'neutral'.\n",
    "- 'model_0_probs': probability distribution predicted by Model 0. The keys are ('positive', 'negative', 'neutral') and the values are floats.\n",
    "- 'text_id': unique identifier for this entry.\n",
    "- 'review_id': review-level identifier for the review from the Yelp Academic Dataset containing 'sentence'.\n",
    "- 'review_rating': review-level star-rating for the review containing 'sentence' in the Yelp Academic Dataset. The possible values are 1, 2, 3, 4, and 5.\n",
    "- 'label_distribution': response distribution from the MTurk validation task. The keys are ('positive', 'negative', 'neutral') and the values are lists of anonymized MTurk ids, which are used consistently throughout the dataset.\n",
    "- 'gold_label': the label chosen by at least three of the five workers if there is one (possible values: 'positive', 'negative', 'neutral', and 'mixed'), else None.\n",
    "\n",
    "- `sentence`: example sentence.\n",
    "- `gold_label`: string output `positive`, `negative`, `neutral` or `mixed`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24890c32",
   "metadata": {},
   "source": [
    "### 2. Preprocess\n",
    "\n",
    "The next step is to load a tokenizer to preprocess the `sentence` field.\n",
    "A tokenizer converts text to a sequence of tokens and creates numerical representation.\n",
    "Notice how there are multiple ways to tokenize text. Make sure to use the right tokenizer for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "53607564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "distilbert-base-uncased\n",
      "['[CLS]', 'hello', 'mr', '.', 'kim', '!', '[SEP]']\n",
      "\n",
      "\n",
      "bert-base-cased\n",
      "['[CLS]', 'Hello', 'Mr', '.', 'Kim', '!', '[SEP]']\n",
      "\n",
      "\n",
      "roberta-base\n",
      "['<s>', 'Hello', 'Ä Mr', '.', 'Kim', '!', '</s>']\n"
     ]
    }
   ],
   "source": [
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "text = \"Hello Mr.Kim!\"\n",
    "\n",
    "for tokenizer in [distilbert_tokenizer, bert_cased_tokenizer, roberta_tokenizer]:\n",
    "  print(f\"\\n\\n{tokenizer.name_or_path}\")\n",
    "  vocab = {v: k for k, v in tokenizer.vocab.items()}\n",
    "  tokenized_text = tokenizer(text)\n",
    "  print([vocab[id] for id in tokenized_text['input_ids']])\n",
    "    \n",
    "tokenizer = distilbert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a5c4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ee30f473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9726d93a01114c74b5555c8999b42dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566af9e4b18f4347bff0a700f0b8b24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933619314af840198b0af1faa9eaa795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_r1_dataset = r1_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4738f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "997470a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce04e7",
   "metadata": {},
   "source": [
    "### 3. Create evaluation method\n",
    "\n",
    "Including a metric during training is often helpful for evaluating your model's performance (otherwise, it just prints the loss). You can quickly load a evaluation method with the ðŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ðŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "26d38612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of correct predictions among the total number of cases processed\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "53367ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "#     print(predicitions, labels)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "42ce2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"positive\", 2:\"neutral\", 3:\"mixed\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1, \"neutral\":2, \"mixed\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf2e6a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This automodel class gives us the model with pretrained weights + a sequence classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=4, id2label=id2label, label2id=label2id\n",
    ")\n",
    "class CustomTrainer(Trainer):\n",
    "    def _inner_training_loop(\n",
    "        self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n",
    "    ):\n",
    "        number_of_epochs = args.num_train_epochs\n",
    "        start = time.time()\n",
    "        train_loss=[]\n",
    "        train_acc=[]\n",
    "        eval_acc=[]\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, gamma=0.9)\n",
    "        \n",
    "        train_dataloader = self.get_train_dataloader()\n",
    "        eval_dataloader = self.get_eval_dataloader()\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss_per_epoch = 0\n",
    "            train_acc_per_epoch = 0\n",
    "            with tqdm(train_dataloader, unit=\"batch\") as training_epoch:\n",
    "                training_epoch.set_description(f\"Training Epoch {epoch}\")\n",
    "                for step, inputs in enumerate(training_epoch):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = inputs['labels']\n",
    "                    \n",
    "                    # forward pass\n",
    "                    self.optimizer.zero_grad()\n",
    "                    output = model(inputs['input_ids'])\n",
    "                    # get the loss\n",
    "                    loss = criterion(output['logits'], labels)\n",
    "                    train_loss_per_epoch += loss.item()\n",
    "                    #calculate gradients\n",
    "                    loss.backward()\n",
    "                    #update weights\n",
    "                    self.optimizer.step()\n",
    "                    train_acc_per_epoch += (output['logits'].argmax(1) == labels).sum().item()\n",
    "            # adjust the learning rate\n",
    "            self.scheduler.step()\n",
    "            train_loss_per_epoch /= len(train_dataloader)\n",
    "            train_acc_per_epoch /= (len(train_dataloader)*batch_size)\n",
    "            \n",
    "            \n",
    "            eval_loss_per_epoch = 0\n",
    "            eval_acc_per_epoch = 0\n",
    "            with tqdm(eval_dataloader, unit=\"batch\") as eval_epoch:\n",
    "                eval_epoch.set_description(f\"Evaluation Epoch {epoch}\")\n",
    "                for step, inputs in enumerate(eval_epoch):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = inputs['labels']\n",
    "                    with torch.no_grad():\n",
    "                        output = model(inputs['input_ids'])\n",
    "                        loss = criterion(output['logits'], labels)\n",
    "                        eval_loss_per_epoch += loss.item()\n",
    "                        eval_acc_per_epoch += (output['logits'].argmax(1) == labels).sum().item()\n",
    "            eval_loss_per_epoch /= (len(eval_dataloader))\n",
    "            eval_acc_per_epoch /= (len(eval_dataloader)*batch_size)\n",
    "        \n",
    "            \n",
    "            print(f'\\tTrain Loss: {train_loss_per_epoch:.3f} | Train Acc: {train_acc_per_epoch*100:.2f}%')\n",
    "            print(f'\\tEval Loss: {eval_loss_per_epoch:.3f} | Eval Acc: {eval_acc_per_epoch*100:.2f}%')\n",
    "    \n",
    "        print(f'Time: {(time.time()-start)/60:.3f} minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d5d0df2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|                                                                    | 0/2516 [00:09<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 4.00 GiB total capacity; 3.27 GiB already allocated; 0 bytes free; 3.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [100], line 25\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# print(training_args)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# https://huggingface.co/transformers/v4.4.2/main_classes/trainer.html#id1\u001b[39;00m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [98], line 33\u001b[0m, in \u001b[0;36mCustomTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 33\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# get the loss\u001b[39;00m\n\u001b[0;32m     35\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m], labels)\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:759\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    757\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 759\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    769\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:579\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:355\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    353\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 355\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:290\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    299\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:217\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    215\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(q, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    216\u001b[0m mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(mask_reshp)\u001b[38;5;241m.\u001b[39mexpand_as(scores)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(weights)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 4.00 GiB total capacity; 3.27 GiB already allocated; 0 bytes free; 3.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/transformers/v4.4.2/main_classes/trainer.html#trainingarguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "# print(training_args)\n",
    "# https://huggingface.co/transformers/v4.4.2/main_classes/trainer.html#id1\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_r1_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_r1_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b748de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af38289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2209a1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e29b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55439a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df05e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f2546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
